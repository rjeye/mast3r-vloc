{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/C45ADD865ADD7620/i3d-rrc/ic-topo-nav/mickst3r\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "\n",
    "# reload notebook automatically after changes to source python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change base folder to parent\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 scenes\n",
      "Selected scene: data/mapfree/val/s00460\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.utils import read_intrinsics\n",
    "\n",
    "dataset_root = Path(\"./data/mapfree/val\")\n",
    "\n",
    "VAL_SCENES = [\"s00460\", \"s00474\", \"s00482\", \"s00489\", \"s00495\"]\n",
    "scene_paths = [dataset_root / scene for scene in VAL_SCENES]\n",
    "\n",
    "print(f\"Found {len(scene_paths)} scenes\")\n",
    "\n",
    "scene_path = scene_paths[0]\n",
    "print(f\"Selected scene: {scene_path}\")\n",
    "\n",
    "intrinsics_path = scene_path / \"intrinsics.txt\"\n",
    "K = read_intrinsics(intrinsics_path, resize=(384, 512)) # resize to model input size\n",
    "\n",
    "reference_image_folder = scene_path / \"seq0\"\n",
    "image_r_path = reference_image_folder / \"frame_00000.jpg\"\n",
    "\n",
    "query_image_folder = scene_path / \"seq1\"\n",
    "query_image_paths = natsorted(list(query_image_folder.glob(\"*.jpg\")))\n",
    "\n",
    "QUERY_NAT_IDX = 1\n",
    "image_q_path = query_image_paths[QUERY_NAT_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "... loading model from ./checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n",
      ">> Loading a list of 2 images\n",
      " - adding data/mapfree/val/s00460/seq0/frame_00000.jpg with resolution 540x720 --> 384x512\n",
      " - adding data/mapfree/val/s00460/seq1/frame_00001.jpg with resolution 540x720 --> 384x512\n",
      " (Found 2 images)\n"
     ]
    }
   ],
   "source": [
    "from mast3r_src.model import AsymmetricMASt3R\n",
    "\n",
    "from dust3r_src.dust3r.inference import inference\n",
    "from dust3r_src.dust3r.utils.image import load_images\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model_name = \"./checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)\n",
    "\n",
    "images = load_images([str(image_r_path), str(image_q_path)], size=512)\n",
    "output_1 = inference([tuple(images)], model, device, batch_size=1, verbose=False)\n",
    "output_2 = inference([tuple(images[::-1])], model, device, batch_size=1, verbose=False)\n",
    "\n",
    "# double mast3r predictions - ðŸ”· schema below\n",
    "# view1, view2 -> dicts with keys 'img', 'true_shape', 'idx', 'instance'\n",
    "# view[\"img\"] -> torch.Tensor of shape (1, 3, H_resize, W_resize) standardized RGB image\n",
    "# view[\"true_shape\"] -> torch.Tensor of shape (1, 2) H_resize, W_resize information\n",
    "# view[\"idx\"] / view[\"instance\"] -> list of length 1 with the int/str index of the image resp.\n",
    "\n",
    "# pred1, pred2 -> dict with keys 'pts3d', 'conf', 'desc', 'desc_conf'\n",
    "# pred[\"pts3d\"] -> torch.Tensor of shape (1, H_resize, W_resize, 3) pointmaps\n",
    "# pred[\"conf\"] / pred[\"desc_conf\"]  -> torch.Tensor of shape (1, H_resize, W_resize) confidence map for pts3d and desc resp.\n",
    "# pred[\"desc\"] -> torch.Tensor of shape (1, H_resize, W_resize, 24) descriptor map\n",
    "view1, pred1 = output_1['view1'], output_1['pred1']\n",
    "view2, pred2 = output_2['view1'], output_2['pred1']\n",
    "\n",
    "# extract confidence from pointmaps of both views\n",
    "conf1, conf2 = pred1['conf'], pred2['conf'] # Shape (1, H_resize, W_resize)\n",
    "print(f\"\\nConfidence shape: {conf1.shape}, {conf2.shape}\")\n",
    "\n",
    "# extract depths from pointmaps of both views as the z-coordinate\n",
    "depth1, depth2 = pred1['pts3d'][..., 2], pred2['pts3d'][..., 2]\n",
    "print(f\"Depths shape: {depth1.shape}, {depth2.shape}\")\n",
    "\n",
    "# extract descriptors from descriptor maps of both views\n",
    "desc1, desc2 = pred1['desc'], pred2['desc']\n",
    "print(f\"Descriptors shape: {desc1.shape}, {desc2.shape}\")\n",
    "\n",
    "# extract descriptors confidence from descriptor maps of both views\n",
    "desc_conf1, desc_conf2 = pred1['desc_conf'], pred2['desc_conf']\n",
    "# print(f\"Descriptors confidence shape: {desc_conf1.shape}, {desc_conf2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Rotation:\n",
      "[[ 0.904065   -0.07803822  0.42021033]\n",
      " [ 0.15057954  0.9783055  -0.14227915]\n",
      " [-0.3999911   0.19190446  0.8962031 ]]\n",
      "\n",
      "Estimated Translation:\n",
      "[-0.85832775 -0.12916309  0.5198691 ]\n",
      "\n",
      "Inliers: 522.5718994140625\n"
     ]
    }
   ],
   "source": [
    "from config.default import cfg\n",
    "cfg.merge_from_file('config/prob_pose.yaml')\n",
    "from src.diff_downsample_maps import downsample_maps_w_kpts\n",
    "\n",
    "from src.diff_probabilistic_procrustes import e2eProbabilisticProcrustesSolver\n",
    "from src.diff_compute_correspondences import ComputeCorrespondences\n",
    "\n",
    "data_batch = downsample_maps_w_kpts(pred1, pred2, target_size=(51, 38), conf_type='conf', device=device)\n",
    "\n",
    "data_batch['K_color0'] = torch.from_numpy(K[f'seq0/{image_r_path.stem}.jpg']).unsqueeze(0).to(device)\n",
    "data_batch['K_color1'] = torch.from_numpy(K[f'seq1/{image_q_path.stem}.jpg']).unsqueeze(0).to(device)\n",
    "\n",
    "compute_Correspondences = ComputeCorrespondences(cfg, device)\n",
    "e2e_Procrustes = e2eProbabilisticProcrustesSolver(cfg)\n",
    "\n",
    "data_batch = compute_Correspondences.prepare_data(data_batch)\n",
    "R_batch, t_batch, inliers_batch, _ = e2e_Procrustes.estimate_pose(\n",
    "    data_batch, return_inliers=True\n",
    ")\n",
    "\n",
    "R_batch = R_batch.squeeze(0).detach().cpu().numpy()\n",
    "t_batch = t_batch.reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "print(f\"Estimated Rotation:\\n{R_batch}\\n\\nEstimated Translation:\\n{t_batch}\\n\\nInliers: {inliers_batch.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_error: 3.367 degrees | t_error: 0.144 meters\n",
      "actual R_delta: 28.902 degrees | t_delta: 0.992 meters\n"
     ]
    }
   ],
   "source": [
    "# read GT pose\n",
    "pose_path = scene_path / \"poses.txt\"\n",
    "# ignore first line and read as image_q_path qw qx qy qz tx ty tz\n",
    "# read as array since first column is the image name\n",
    "pose = np.loadtxt(pose_path, skiprows=2, dtype=str)\n",
    "gt_name = pose[QUERY_NAT_IDX, 0]\n",
    "gt_qvec = np.array(pose[QUERY_NAT_IDX, 1:5], dtype=float)\n",
    "gt_tvec = np.array(pose[QUERY_NAT_IDX, 5:], dtype=float)\n",
    "\n",
    "from src.tf_utils import compose_qt_tf, calculate_rot_error, calculate_translation_error\n",
    "\n",
    "gt_R, gt_t = compose_qt_tf(gt_qvec, gt_tvec, return_Rt=True)\n",
    "\n",
    "R_error = calculate_rot_error(gt_R, R_batch)\n",
    "t_error = calculate_translation_error(gt_t, t_batch)\n",
    "print(f\"R_error: {round(R_error, 3)} degrees | t_error: {round(t_error, 3)} meters\")\n",
    "\n",
    "R_delta = calculate_rot_error(gt_R, np.eye(3))\n",
    "t_delta = calculate_translation_error(gt_t, np.zeros(3))\n",
    "print(f\"actual R_delta: {round(R_delta, 3)} degrees | t_delta: {round(t_delta, 3)} meters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mickey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
