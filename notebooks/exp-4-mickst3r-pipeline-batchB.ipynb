{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/C45ADD865ADD7620/i3d-rrc/ic-topo-nav/mickst3r\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "\n",
    "# reload notebook automatically after changes to source python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# change base folder to parent\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "... loading model from ./checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\n",
      "instantiating : AsymmetricMASt3R(enc_depth=24, dec_depth=12, enc_embed_dim=1024, dec_embed_dim=768, enc_num_heads=16, dec_num_heads=12, pos_embed='RoPE100',img_size=(512, 512), head_type='catmlp+dpt', output_mode='pts3d+desc24', depth_mode=('exp', -inf, inf), conf_mode=('exp', 1, inf), patch_embed_cls='PatchEmbedDust3R', two_confs=True, desc_conf_mode=('exp', 0, inf), landscape_only=False)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from mast3r_src.model import AsymmetricMASt3R\n",
    "from dust3r_src.dust3r.inference import inference\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model_name = \"./checkpoints/MASt3R_ViTLarge_BaseDecoder_512_catmlpdpt_metric.pth\"\n",
    "model = AsymmetricMASt3R.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 scenes\n",
      "Selected scene: data/mapfree/val/s00460\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.utils import read_intrinsics\n",
    "from dust3r_src.dust3r.utils.image import load_images\n",
    "\n",
    "dataset_root = Path(\"./data/mapfree/val\")\n",
    "\n",
    "VAL_SCENES = [\"s00460\", \"s00474\", \"s00482\", \"s00489\", \"s00495\"]\n",
    "scene_paths = [dataset_root / scene for scene in VAL_SCENES]\n",
    "print(f\"Found {len(scene_paths)} scenes\")\n",
    "\n",
    "scene_path = scene_paths[0]\n",
    "print(f\"Selected scene: {scene_path}\")\n",
    "\n",
    "intrinsics_path = scene_path / \"intrinsics.txt\"\n",
    "K = read_intrinsics(intrinsics_path, resize=(384, 512)) # resize to model input size\n",
    "\n",
    "reference_image_folder = scene_path / \"seq0\"\n",
    "ref_image_path = str(reference_image_folder / \"frame_00000.jpg\")\n",
    "\n",
    "DOWNSAMPLE = 20\n",
    "query_image_folder = scene_path / \"seq1\"\n",
    "query_image_paths = natsorted(list(str(p) for p in query_image_folder.glob(\"*.jpg\")))[::DOWNSAMPLE]\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "query_image_paths = query_image_paths[:BATCH_SIZE]\n",
    "\n",
    "query_images = load_images(query_image_paths, size=512, verbose=False)\n",
    "reference_image = load_images([ref_image_path], size=512, verbose=False)[0]\n",
    "\n",
    "pairs = [(reference_image, query_image) for query_image in query_images]\n",
    "pairs_rev = [(query_image, reference_image) for query_image in query_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence shape: torch.Size([8, 512, 384]), torch.Size([8, 512, 384])\n",
      "Depths shape: torch.Size([8, 512, 384]), torch.Size([8, 512, 384])\n",
      "Descriptors shape: torch.Size([8, 512, 384, 24]), torch.Size([8, 512, 384, 24])\n"
     ]
    }
   ],
   "source": [
    "output_1 = inference(pairs, model, device, batch_size=BATCH_SIZE, verbose=False)\n",
    "output_2 = inference(pairs_rev, model, device, batch_size=BATCH_SIZE, verbose=False)\n",
    "\n",
    "view1, pred1 = output_1['view1'], output_1['pred1']\n",
    "view2, pred2 = output_2['view1'], output_2['pred1']\n",
    "\n",
    "# extract confidence from pointmaps of both views\n",
    "conf1, conf2 = pred1['conf'], pred2['conf'] # Shape (1, H_resize, W_resize)\n",
    "print(f\"\\nConfidence shape: {conf1.shape}, {conf2.shape}\")\n",
    "\n",
    "# extract depths from pointmaps of both views as the z-coordinate\n",
    "depth1, depth2 = pred1['pts3d'][..., 2], pred2['pts3d'][..., 2]\n",
    "print(f\"Depths shape: {depth1.shape}, {depth2.shape}\")\n",
    "\n",
    "# extract descriptors from descriptor maps of both views\n",
    "desc1, desc2 = pred1['desc'], pred2['desc']\n",
    "print(f\"Descriptors shape: {desc1.shape}, {desc2.shape}\")\n",
    "\n",
    "# extract descriptors confidence from descriptor maps of both views\n",
    "desc_conf1, desc_conf2 = pred1['desc_conf'], pred2['desc_conf']\n",
    "# print(f\"Descriptors confidence shape: {desc_conf1.shape}, {desc_conf2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.default import cfg\n",
    "cfg.merge_from_file('config/prob_pose.yaml')\n",
    "from src.diff_downsample_maps import downsample_maps_w_kpts\n",
    "\n",
    "from src.diff_probabilistic_procrustes import e2eProbabilisticProcrustesSolver\n",
    "from src.diff_compute_correspondences import ComputeCorrespondences\n",
    "\n",
    "data_batch = downsample_maps_w_kpts(pred1, pred2, target_size=(51, 38), conf_type='conf', device=device)\n",
    "\n",
    "# for K_color_0 read intrinsics of reference image and create a 8, 3, 3 tensor\n",
    "K_color0 = torch.from_numpy(K[f'seq0/{Path(ref_image_path).stem}.jpg']).unsqueeze(0).to(device)\n",
    "K_color0_batch = K_color0.repeat(conf1.shape[0], 1, 1)\n",
    "\n",
    "# for K_color_1 read intrinsics of all 8 query image and create a 8, 3, 3 tensor\n",
    "K_color1_batch = torch.stack([torch.from_numpy(K[f'seq1/{Path(image_q_path).stem}.jpg']) for image_q_path in query_image_paths]).to(device)\n",
    "\n",
    "data_batch['K_color0'] = K_color0_batch\n",
    "data_batch['K_color1'] = K_color1_batch\n",
    "\n",
    "compute_Correspondences = ComputeCorrespondences(cfg, device)\n",
    "e2e_Procrustes = e2eProbabilisticProcrustesSolver(cfg)\n",
    "\n",
    "data_batch = compute_Correspondences.prepare_data(data_batch)\n",
    "R_batch, t_batch, inliers_batch, _ = e2e_Procrustes.estimate_pose(\n",
    "    data_batch, return_inliers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated R:\n",
      "[[ 0.9082328  -0.1075771   0.4037031 ]\n",
      " [ 0.17362139  0.976099   -0.13105366]\n",
      " [-0.38005912  0.18915448  0.90546274]]\n",
      ", t=[-0.84435964 -0.15628873  0.48315394], inliers=497.6474609375\n"
     ]
    }
   ],
   "source": [
    "R = R_batch[0].unsqueeze(0).detach().cpu().numpy()\n",
    "t = t_batch[0].reshape(-1).detach().cpu().numpy()\n",
    "inliers = inliers_batch[0].item()\n",
    "\n",
    "print(f\"Estimated R:\\n{R}\\n, t={t}, inliers={inliers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mickey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
